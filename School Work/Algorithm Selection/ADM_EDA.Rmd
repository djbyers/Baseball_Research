---
title: "Predicting the Outcome of MLB Games through Statistical Modeling"
author: "Douglas Byers, Matt Muller, TJ Rogers"
date: "11/2/2020"
output:
  pdf_document:
    fig_height: 2
    fig_width: 3.5
  html_document: default
editor_options: 
  chunk_output_type: console
---
# 1) Setup and Load Packages, Libraries, and Data
```{r setup, include=FALSE, echo = FALSE, messages = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
 (library(readr))
 (library(mosaic))
 (library(tidyverse))
 (library(glmnet))
 (library(class))
 (library(modelr))
 (library(lubridate))
 (library(caret))
 (library(vip)) 
 (library(gbm))
 (library(purrr))
library(randomForest)
```

# 2) Data scraped from Baseball Reference

BaseballReference is the leading online database for baseball statistics, ranging from overall numbers and situational splits to individual game by game statistics. Since the goal of this project is to predict the outcome of an individual game, we chose to scrape our data from the individual game logs. Our dataset includes offensive and defensive/pitching statistics for each game every year. Since 2019 was the last season in which a full 162 game schedule was played, we decided to use the 2019 season as our test dataset, with the 2018 season serving as our training dataset.

Since there were a few predictive statistics not included in the BaseballReference database, we were able to create our scraping function in such a way to build in the creation of those variables. We had two separate, but very similar, scraping functions. The function "scraping" was used to obtain the offensive statistics, with "pitching" serving as our way of getting the defensive/pitching statistics:

```{r, eval = FALSE}
scraping <- function (url) {
  url1 <- read_html(url)
  nodes <- html_nodes(url1, css = "table")
  table <- html_table(nodes, header = TRUE, fill = TRUE)
  tibble <- as.data.frame(table)
  tibble <- rename(tibble, "2B" = "X2B", "3B" = "X3B")%>%
    filter(Opp != "Opp")
  tibble <- tibble %>%
    mutate(SOPercH = cumsum(SO)/cumsum(PA))
  tibble
}

pitching <- function (url) {
  url1 <- read_html(url)
  nodes <- html_nodes(url1, css = "table")
  table <- html_table(nodes, header = TRUE, fill = TRUE)
  tibble <- as.data.frame(table[[2]])
  mutated <- rename(tibble, c("HA" = "H", "RA" = "R", "BBA" = "BB", "OSO" = "SO",
                              "HRA" = "HR", "SBA" = "SB", "CSA" = "CS", "ABA" = "AB",
                              "2BA" = "2B", "3BA" = "3B", "SHA" = "SH", "SFA" = "SF", 
                              "." = "","ROEA" = "ROE", "HBPA" = "HBP", "DPT" = "GDP",
                              "IBBA" = "IBB", "NumPitchers" = "#"))%>%
    filter(Opp != "Opp")
  mutated <- mutated%>%
    select(7:34)%>%
    mutate(BAA = cumsum(as.numeric(HA))/cumsum(as.numeric(ABA)),
           SLGA = (4 * cumsum(as.numeric(HRA)) + 3 * cumsum(as.numeric(`3BA`)) + 2 * cumsum(as.numeric(`2BA`)) + 
                     (cumsum(as.numeric(HA)) - (cumsum(as.numeric(`2BA`)) + cumsum(as.numeric(`3BA`))
                                                + cumsum(as.numeric(HRA)))))/cumsum(as.numeric(ABA)),
           OBPA = (cumsum(as.numeric(HA)) + cumsum(as.numeric(BBA)) + cumsum(as.numeric(HBPA)))/(cumsum(as.numeric(ABA)) + 
                      cumsum(as.numeric(BBA)) + cumsum(as.numeric(HBPA)) + cumsum(as.numeric(SFA)) + cumsum(as.numeric(SHA))),
           OPSA = as.numeric(SLGA) + as.numeric(OBPA),
           WHIP = (cumsum(HA)+ cumsum(BBA))/cumsum(IP),
           FIP = ((13 * cumsum(HRA) + 3 * cumsum(BBA) - 2 * cumsum(OSO))/cumsum(IP)) + 3.214,
           StrPerc = cumsum(Str)/cumsum(Pit),
           SOPerc = cumsum(OSO)/cumsum(BF),
           K_9 = (cumsum(OSO) * 9)/cumsum(IP),
           K_BB = cumsum(OSO)/(cumsum(BBA) + 0.01),
           HR_9 = (cumsum(HRA) * 9)/cumsum(IP))
  
  mutated
}
```



```{r, include = FALSE, warning = FALSE}
games_2019 <- read_csv("~/Mscs 341b S21/Project/Matt_TJ_Doug/games_2019.csv")
games_2018 <- read_csv("~/Mscs 341b S21/Project/Matt_TJ_Doug/games_2018.csv")
```

# 3) Data manipulation, refining the datasets

Since our regressions and machine learning algorithms will be iterating over date to be able to predict the outcome of a particular game, we need to change our dataset to ignore games that are the second game being played between the same two teams on the same day (Game 2 of a double header). Also, since we want to predict the outcome of a particular game, a Win or not, as a binary response variable, we will add a variable for Win, with values 1 and 0, 1 representing a Win for that particular team.

```{r, warning = FALSE}
games_2018 <- games_2018%>%
  separate(Date, into = c("Mth", "Day"), sep = " ")%>%
  mutate(Month = if_else(Mth == "Mar", 3, 
                   if_else(Mth == "Apr", 4,
                     if_else(Mth == "May", 5,
                        if_else(Mth == "Jun", 6,
                           if_else(Mth == "Jul", 7,
                              if_else(Mth == "Aug", 8,
                                if_else(Mth == "Sep", 9, 10))))))))%>%
  unite(Date, Month, Day, Year, sep = "/")

games.train <- games_2018 %>%
  mutate(Home = if_else(is.na(Var.4), 1, 0))%>%
  separate(Rslt, into = c("Result", "Score"), sep = ",")%>%
  separate(Score, into = c("Runs Scored", "Runs Allowed"), sep = "-") %>%
  mutate(Home_Team = if_else(Home == 1, team, Opp))%>%
  mutate(Away_Team = if_else(Home == 0, team, Opp))%>%
  mutate(Matchup_1 = Home_Team)%>%
  mutate(Matchup_2 = Away_Team)%>%
  unite(Matchup, Matchup_1, Matchup_2, sep = ",")%>%
  mutate(Run_Diff = as.numeric(`Runs Scored`) - as.numeric(`Runs Allowed`))%>%
  mutate(Win = ifelse(Result == "W", 1, 0))%>%
  arrange(mdy(Date), Matchup, Home)

g1 <- games.train%>%
  filter(team == lag(team) & Opp == lag(Opp))

games.train <- setdiff(games.train, g1)

games_2019 <- games_2019%>%
  separate(Date, into = c("Mth", "Day"), sep = " ")%>%
  mutate(Month = if_else(Mth == "Mar", 3, 
                   if_else(Mth == "Apr", 4,
                     if_else(Mth == "May", 5,
                        if_else(Mth == "Jun", 6,
                           if_else(Mth == "Jul", 7,
                              if_else(Mth == "Aug", 8,
                                if_else(Mth == "Sep", 9, 10))))))))%>%
  unite(Date, Month, Day, Year, sep = "/")

games.test <- games_2019 %>%
  mutate(Home = if_else(is.na(Var.4), 1, 0))%>%
  separate(Rslt, into = c("Result", "Score"), sep = ",")%>%
  separate(Score, into = c("Runs Scored", "Runs Allowed"), sep = "-") %>%
  mutate(Home_Team = if_else(Home == 1, team, Opp))%>%
  mutate(Away_Team = if_else(Home == 0, team, Opp))%>%
  mutate(Matchup_1 = Home_Team)%>%
  mutate(Matchup_2 = Away_Team)%>%
  unite(Matchup, Matchup_1, Matchup_2, sep = ",")%>%
  mutate(Run_Diff = as.numeric(`Runs Scored`) - as.numeric(`Runs Allowed`))%>%
  mutate(Win = ifelse(Result == "W", 1, 0))%>%
  arrange(mdy(Date), Matchup, Home)


g2 <- games.test %>%
  filter(team == lag(team) & Opp == lag(Opp))

games.test <- setdiff(games.test, g2)


cols.num <- c(8:32, 36:62, 65:75, 77, 81)
games.train[cols.num] <- sapply(games.train[cols.num],as.numeric)
games.test[cols.num] <- sapply(games.test[cols.num],as.numeric)

```

# 4) In Game Statistical Analysis
 *  First we want to run exploratory analysis and models for games with how teams perform in a given game. This should lead to models with a very low error rate since teams with better offensive and defensive statistics in a particular game are clearly more likely to win. In this way, we can see benchmarks for each statistic that teams can strive for when going into a game.

## 4.a) Variable Plots for EDA
 *  We run box and scatter plots to compare statistics we think may have an effect on a game with the outcome (win or loss).
```{r}
(RBox = ggplot (games.test, aes(y=as.factor(Win), x=R))+
  geom_boxplot())

(RABox = ggplot (games.test, aes(y=as.factor(Win), x=RA))+
  geom_boxplot())

(HBox = ggplot (games.test, aes(y=as.factor(Win), x=H))+
  geom_boxplot())

(HRBox = ggplot (games.test, aes(y=as.factor(Win), x=HR))+
  geom_boxplot())

(OBPBox = ggplot (games.test, aes(y=as.factor(Win), x=OBP))+
  geom_boxplot())

(SLGBox = ggplot (games.test, aes(y=as.factor(Win), x=SLG))+
  geom_boxplot())

(OPSBox = ggplot (games.test, aes(y=as.factor(Win), x=OPS))+
  geom_boxplot())

(HABox = ggplot (games.test, aes(y=as.factor(Win), x=HA))+
  geom_boxplot())

(BBABox = ggplot (games.test, aes(y=as.factor(Win), x=BBA))+
  geom_boxplot())

(HRABox = ggplot (games.test, aes(y=as.factor(Win), x=HRA))+
  geom_boxplot())

(StrPBox = ggplot (games.test, aes(y=as.factor(Win), x=StrPerc))+
  geom_boxplot())

(OPSABox = ggplot (games.test, aes(y=as.factor(Win), x=OPSA))+
  geom_boxplot())

(BAABox = ggplot (games.test, aes(y=as.factor(Win), x=BAA))+
  geom_boxplot())

(SOPercBox = ggplot (games.test, aes(y=as.factor(Win), x=SOPerc))+
  geom_boxplot())

(SOPercHBox = ggplot (games.test, aes(y=as.factor(Win), x=SOPercH))+
  geom_boxplot())
```

 *  We notice clear separation in Hits, Homeruns, Walks, and Runs, which is likely due to the fact that these are continuous and not percentage variables. It appears that On Base Percentage has the best separation for percentage variables that we displayed.

```{r}
(BA_BAAplot = ggplot(games.test)+
  geom_point(aes(x= BA, y = BAA, color = as.factor(Win)), alpha = 0.4))

(SOPplot = ggplot(games.test)+
  geom_point(aes(x= SOPercH, y = SOPerc, color = as.factor(Win)), alpha = 0.4))
```

 *  There is not great separation between either of these offensive/defensive combinations, but slightly better significance shown through Batting Average and Batting Average Against.

## 4.b) Example Models
 *  As a mini test run, we look to run some models with a restricted amount of variables that we think may be key factors in predicting whether a team wins or not. We'll do this with two simple models, linear and logistic regression. The categorical variable Win (whether the team wins or not) is the response.

### 4.b.i) Linear Model

```{r}
mod.lm.samp <- lm(Win ~ OBP + SOPerc + H + HR + WHIP + HRA, 
             data = games.train)

games.test.samp <- games.test %>%
  add_predictions(mod.lm.samp,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0))

table(games.test.samp$Win, games.test.samp$class)
(err.lm.samp <- with(games.test.samp, mean(Win != class)))
```

```{r}
summary(mod.lm.samp)
```

 *  Error rate of 22.5% is quite solid when choosing random explanatory variables, this should be reduced through unrestricted models later. The R-Squared value of 34.57% can definitely be improved.

### 4.b.ii) Log Model

```{r}
mod.log.samp <- glm(Win ~ OBP + SOPerc + H + HR + WHIP + HRA, 
             data = games.train,
             family = "binomial")

games.test.samp.log <- games.test %>%
  add_predictions(mod.log.samp,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0)) #class instead of response

table(games.test.samp.log$Win, games.test.samp.log$class)
(err.log.samp <- with(games.test.samp.log, mean(Win != class)))
```

```{r}
summary(mod.log.samp)
```

 *  We notice a similar error rate at 22.6% with the log model.

## 4.c) Unrestricted models
 *  We now look to examine models including all variables as the explanatory variables (other than the categorical variable Win as the response).

### 4.c.i) Lasso Model
 *  Now we'll work on creating a lasso model. First we want to make sure all variables included are numeric and that the data is represented as a matrix.
```{r}
games.y <- data.matrix(games.train$Win)


games.x.lasso <- games.train %>%
  select(8:32, 36:62, 65:75, 77)
games.x.lasso[is.na(games.x.lasso)] <- 0
games.x <- data.matrix(games.x.lasso)
```

```{r}
lasso.cv <- cv.glmnet(games.x,
                      games.y,
                      family="binomial",
                      type.measure="class", 
                      alpha=1)

plot(lasso.cv)
```

```{r}
vip(lasso.cv)
```

```{r}
lambda.opt <- lasso.cv$lambda.1se
id <- with(lasso.cv,which(lasso.cv$lambda==lambda.opt))
(err.lasso <- lasso.cv$cvm[id])
```

 *  We notice an optimal lambda around .25 with 4 variables included. The error is 0 because clearly the team with more runs will win, and the model is reduced to only variables where "runs" is present.

 *  Obviously, RBI's and runs/runs against will have an impact on the outcome. These statistics refer to the amount of runs scored and given up by a team, which clearly directly correlate to wins. Let's look at more vague statistics to see how well they predict the model without these. Thus, we'll narrow down our explanatory variables even more by taking out run related variables.

```{r}
games.test.y <- data.matrix(games.test$Win)
games.test.x1 <- games.test %>%
  select(10:11, 13:16, 18:32, 36:38, 42:62, 65:75, 77)
games.test.x1[is.na(games.test.x1)] <- 0
games.test.x <- data.matrix(games.test.x1)
```

```{r}
games.x.noR1 <- games.train %>%
  select(10:11, 13:16, 18:32, 36:38, 42:62, 65:75, 77)
games.x.noR1[is.na(games.x.noR1)] <- 0
games.x1 <- data.matrix(games.x.noR1)
```


 *  We'll run another lasso model with a slightly reduced data matrix.

```{r}
lasso.cv1 <- cv.glmnet(games.x1,
                      games.y,
                      family="binomial",
                      type.measure="class", 
                      alpha=1)

plot(lasso.cv1)
```

```{r}
vip(lasso.cv1)
```

```{r}
res.vip1 <- vip(lasso.cv1,num_features=55)
vip.dat1 <- res.vip1[["data"]]
vip.dat1[52:55,]
```

```{r}
lambda.opt <- lasso.cv1$lambda.1se
id1 <- with(lasso.cv1,which(lasso.cv1$lambda==lambda.opt))
(err.lasso1 <- lasso.cv1$cvm[id1])
```

```{r}
preds.l <- predict(lasso.cv1, newx=games.x1,
                 type="class")
##The error rate on the testing dataset
table(games.y, preds.l)
```

 *  Here the optimal lambda is around 0 and there's an error rate of about 2%. This is a great error rate likely due to the reduction of multicollinearity when limiting the amount of predictors. Note that the error rate may differ from trial to trial due to cross validation

 *  Innings Pitched, whether a team is Home, and Slugging Percentage have a lot of relative importance, with Hits, Reach on Error, and Grounding into Double Plays (offensively) follow quite close behind.

### 4.c.ii) Ridge Model

 *  We'll use the same smaller dataset that we used for lasso for ridge as well (numeric explanatory variables) to predict a ridge model, along with an optimal lambda.

```{r}
ridge.cv1 <- cv.glmnet(games.x1, games.y,
                      family="binomial",
                      type.measure="class", 
                      alpha=0)
plot(ridge.cv1)
```

```{r}
log(ridge.cv1$lambda.1se)
log(ridge.cv1$lambda.1se)
```

```{r}
lambda.opt1 <- ridge.cv1$lambda.1se
id2 <- with(ridge.cv1,which(ridge.cv1$lambda==lambda.opt1))
(err.ridge1 <- ridge.cv1$cvm[id2])
```

```{r}
preds.r <- predict(ridge.cv1, newx=games.test.x,
                 type="class")
##The error rate on the testing dataset
table(games.test.y ,preds.r)
```

```{r}
vip(ridge.cv1)
```

```{r}
res.vip <- vip(ridge.cv1,num_features=55)
vip.dat <- res.vip[["data"]]
vip.dat[52:55,]
```

 *  With an optimal lambda of about -3.8, we notice an MSE of 7.3%, which is much worse than the lasso model likely due to multicollinearity from the similarity of some of these variables in predicting Wins.

 *  Interesting, we actually see no statistic representing Runs through the ridge model which is pretty cool. The greatest importance seems to be represented by specific pitching and hitting stats that wouldn't directly correlate with wins at first sight (which runs and runs against clearly would). In the ridge model, Batting Average, Slugging Percentage, Strikeout Percentage, and On Base Percentage show the most significance to our model.

 *  Note once again that results may slightly vary from trial to trial due to the process of cross validation.

### 4.c.iii) Linear and Log models unrestricted

Now that we've seen these, let's try linear and log models with the reduced variables (numeric ones) that we've used.

```{r}
##New data set, gotta adjust the numbers selected
games.test1 <- games.test %>%
  select(Win, 10:11, 13:16, 18:32, 36:38, 42:62, 65:75, 77)

games.train1 <- games.train %>%
  select(Win, 10:11, 13:16, 18:32, 36:38, 42:62, 65:75, 77)

games.train1[is.na(games.train1)] <- 0
games.test1[is.na(games.test1)] <- 0
```


```{r}
mod.lm <- lm(Win ~ ., 
             data = games.train1)

games.test.lm <- games.test1 %>%
  add_predictions(mod.lm,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0))

table(games.test.lm$Win, games.test.lm$class)
(err.lm <- with(games.test.lm, mean(Win != class)))
```

```{r}
vip(mod.lm)
```

```{r}
summary(mod.lm)
```

```{r}
res.vip2 <- vip(mod.lm,num_features=55)
vip.dat2 <- res.vip2[["data"]]
vip.dat2[52:55,]
```

 *  Our linear model shows a comparatively competitive MSE with the lasso model at 2.46%. Hits, Home, and Grounding into Double Plays interestingly enough prove to be the most significant variables in this model. Further, we see an extremely improved R-Squared from our example linear model to 85.42%

```{r}
mod.log <- glm(Win ~ ., 
             data = games.train1,
             family = "binomial")

games.test.log <- games.test1 %>%
  add_predictions(mod.log,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0))

table(games.test.log$Win, games.test.log$class)
(err.log <- with(games.test.log, mean(Win != class)))
```

```{r}
vip(mod.log)
```

```{r}
summary(mod.log)
```

```{r}
res.vip3 <- vip(mod.log,num_features=55)
vip.dat3 <- res.vip3[["data"]]
vip.dat3[52:55,]
```

 *  The logistic model shows a slightly better MSE than the linear model at 2.36%. The important variables in this model are quite similar to the linear significant variables.

# 5) Cumulative Team Statistic Analysis

 *  Now we reach a point where we're ready to test models by using cumulative team statistics leading up to the predicted game. We use percentage statistics for these models to compare averages of the two different teams playing and separate variables into Home and Away Statistics by creating a new dataset from our previous one.

## 5.a) Data Scraping and Manipulation Part II

In our previous dataset, each game was represented by two separate rows, one with the predictors for each team. We want to have our dataset such that each game is fully encompassed by one row. To do that, we change our response variable to be whether or not the Home Team wins. Not only is this an easy way of organizing the data, as now we can have each predictor classified by whether or not it belongs to the Home Team or the Away Team, this new model also conveniently builds home field advantage into the model.

```{r, include=FALSE}
games.train2 <- games.train%>%
  mutate(Home_Win = if_else(Home == 1 & Win == 1 | Home == 0 & Win == 0, 1, 0),
         Home_BA = if_else(Home == 0, lead(BA), BA),
         Away_BA = if_else(Home == 1, lag(BA), BA),
         Home_OBP = if_else(Home == 0, lead(OBP), OBP),
         Away_OBP = if_else(Home == 1, lag(OBP), OBP),
         Home_SLG = if_else(Home == 0, lead(SLG), SLG),
         Away_SLG = if_else(Home == 1, lag(SLG), SLG),
         Home_OPS = if_else(Home == 0, lead(OPS), OPS),
         Away_OPS = if_else(Home == 1, lag(OPS), OPS),
         Home_ISO = Home_SLG - Home_BA,
         Away_ISO = Away_SLG - Away_BA,
         Home_BAA = if_else(Home == 0, lead(BAA), BAA),
         Away_BAA = if_else(Home == 1, lag(BAA), BAA),
         Home_OBPA = if_else(Home == 0, lead(OBPA), OBPA),
         Away_OBPA = if_else(Home == 1, lag(OBPA), OBPA),
         Home_SLGA = if_else(Home == 0, lead(SLGA), SLGA),
         Away_SLGA = if_else(Home == 1, lag(SLGA), SLGA),
         Home_OPSA = if_else(Home == 0, lead(OPSA), OPSA),
         Away_OPSA = if_else(Home == 1, lag(OPSA), OPSA),
         Home_StrPerc = if_else(Home == 0, lead(StrPerc), StrPerc),
         Away_StrPerc = if_else(Home == 1, lag(StrPerc), StrPerc),
         Home_ERA = if_else(Home == 0, lead(ERA), ERA),
         Away_ERA = if_else(Home == 1, lag(ERA), ERA),
         Home_WHIP = if_else(Home == 0, lead(WHIP), WHIP),
         Away_WHIP = if_else(Home == 1, lag(WHIP), WHIP),
         Home_FIP = if_else(Home == 0, lead(FIP), FIP),
         Away_FIP = if_else(Home == 1, lag(FIP), FIP),
         Home_SOPerc = if_else(Home == 0, lead(SOPerc), SOPerc),
         Away_SOPerc = if_else(Home == 1, lag(SOPerc), SOPerc),
         Home_SOPercH = if_else(Home == 0, lead(SOPercH), SOPercH),
         Away_SOPercH = if_else(Home == 1, lag(SOPercH), SOPercH),
         Home_K9 = if_else(Home == 0, lead(K_9), K_9),
         Away_K9 = if_else(Home == 1, lag(K_9), K_9),
         Home_KBB = if_else(Home == 0, lead(K_BB), K_BB),
         Away_KBB = if_else(Home == 1, lag(K_BB), K_BB),
         Home_HR9 = if_else(Home == 0, lead(HR_9), HR_9),
         Away_HR9 = if_else(Home == 1, lag(HR_9), HR_9),
         Home_RunDiff = if_else(Home == 0, lead(Run_Diff), Run_Diff))%>%
  select(Date, Matchup, Home_Team, Home_BA, Home_OBP, Home_SLG, Home_OPS, Home_ISO, Home_SOPercH, Home_BAA, Home_OBPA, Home_SLGA, Home_OPSA, Home_StrPerc, Home_ERA, Home_WHIP, Home_FIP, Home_SOPerc, Home_KBB, Home_K9, Home_HR9, Away_Team, Away_BA, Away_OBP, Away_SLG, Away_OPS, Away_ISO, Away_SOPercH, Away_BAA, Away_OBPA, Away_SLGA, Away_OPSA, Away_StrPerc, Away_ERA, Away_WHIP, Away_FIP, Away_SOPerc, Away_KBB, Away_K9, Away_HR9, Home_Win, Home_RunDiff)%>%
  unique
```


```{r, include = FALSE}
games.test2 <- games.test%>%
  mutate(Home_Win = if_else(Home == 1 & Win == 1 | Home == 0 & Win == 0, 1, 0),
         Home_BA = if_else(Home == 0, lead(BA), BA),
         Away_BA = if_else(Home == 1, lag(BA), BA),
         Home_OBP = if_else(Home == 0, lead(OBP), OBP),
         Away_OBP = if_else(Home == 1, lag(OBP), OBP),
         Home_SLG = if_else(Home == 0, lead(SLG), SLG),
         Away_SLG = if_else(Home == 1, lag(SLG), SLG),
         Home_OPS = if_else(Home == 0, lead(OPS), OPS),
         Away_OPS = if_else(Home == 1, lag(OPS), OPS),
         Home_ISO = Home_SLG - Home_BA,
         Away_ISO = Away_SLG - Away_BA,
         Home_BAA = if_else(Home == 0, lead(BAA), BAA),
         Away_BAA = if_else(Home == 1, lag(BAA), BAA),
         Home_OBPA = if_else(Home == 0, lead(OBPA), OBPA),
         Away_OBPA = if_else(Home == 1, lag(OBPA), OBPA),
         Home_SLGA = if_else(Home == 0, lead(SLGA), SLGA),
         Away_SLGA = if_else(Home == 1, lag(SLGA), SLGA),
         Home_OPSA = if_else(Home == 0, lead(OPSA), OPSA),
         Away_OPSA = if_else(Home == 1, lag(OPSA), OPSA),
         Home_StrPerc = if_else(Home == 0, lead(StrPerc), StrPerc),
         Away_StrPerc = if_else(Home == 1, lag(StrPerc), StrPerc),
         Home_ERA = if_else(Home == 0, lead(ERA), ERA),
         Away_ERA = if_else(Home == 1, lag(ERA), ERA),
         Home_WHIP = if_else(Home == 0, lead(WHIP), WHIP),
         Away_WHIP = if_else(Home == 1, lag(WHIP), WHIP),
         Home_FIP = if_else(Home == 0, lead(FIP), FIP),
         Away_FIP = if_else(Home == 1, lag(FIP), FIP),
         Home_SOPerc = if_else(Home == 0, lead(SOPerc), SOPerc),
         Away_SOPerc = if_else(Home == 1, lag(SOPerc), SOPerc),
         Home_SOPercH = if_else(Home == 0, lead(SOPercH), SOPercH),
         Away_SOPercH = if_else(Home == 1, lag(SOPercH), SOPercH),
         Home_K9 = if_else(Home == 0, lead(K_9), K_9),
         Away_K9 = if_else(Home == 1, lag(K_9), K_9),
         Home_KBB = if_else(Home == 0, lead(K_BB), K_BB),
         Away_KBB = if_else(Home == 1, lag(K_BB), K_BB),
         Home_HR9 = if_else(Home == 0, lead(HR_9), HR_9),
         Away_HR9 = if_else(Home == 1, lag(HR_9), HR_9),
         Home_RunDiff = if_else(Home == 0, lead(Run_Diff), Run_Diff))%>%
  select(Date, Matchup, Home_Team, Home_BA, Home_OBP, Home_SLG, Home_OPS, Home_ISO, Home_SOPercH, Home_BAA, Home_OBPA, Home_SLGA, Home_OPSA, Home_StrPerc, Home_ERA, Home_WHIP, Home_FIP, Home_SOPerc, Home_KBB, Home_K9, Home_HR9, Away_Team, Away_BA, Away_OBP, Away_SLG, Away_OPS, Away_ISO, Away_SOPercH, Away_BAA, Away_OBPA, Away_SLGA, Away_OPSA, Away_StrPerc, Away_ERA, Away_WHIP, Away_FIP, Away_SOPerc, Away_KBB, Away_K9, Away_HR9, Home_Win, Home_RunDiff)%>%
  unique
```


```{r, include = FALSE}
games.train3 <- games.train2 %>%
  select(4:21, 23:41)

games.test3 <- games.test2 %>%
  select(4:21, 23:41)
```

## 5.b) Modeling and Analysis

 *  We're looking for error rates between 38% and 42% for these models as some of the best professional predictive and betting models show success rates between 58% - 62%. With these models, we can use a team's current statistics at a given point in the season to predict which team will win a game. This is a more realistic prediction method in general or when considering sports betting, since it's impossible to know the in game statistics before the game is played.

### 5.b.i) Logistic Model

 *  As before, we will begin with fitting simple linear and logistic models.

```{r}
mod.log.cumu <- glm(Home_Win ~.,
               data = games.train3,
               family = "binomial")

games.test.log <- games.test3 %>%
  add_predictions(mod.log.cumu,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0)) #class instead of response

table(games.test.log$Home_Win, games.test.log$class)
(err.log.cumu <- with(games.test.log, mean(Home_Win != class)))
```

```{r}
vip(mod.log.cumu)
```

```{r}
res.vip4 <- vip(mod.log.cumu,num_features=55)
vip.dat4 <- res.vip4[["data"]]
vip.dat4[29:32,]
```

```{r}
summary(mod.log.cumu)
```

 *  The logistic regression model displays an error rate of 39.6%, which is right within our desired range. The most significant variables include Home Strike Percentage, Offensive Strikeout Percentage, and Earned Run Average.

### 5.b.ii) Linear Model

```{r}
mod.lm.cumu <- lm(Home_Win ~.,
               data = games.train3)
```

```{r}
games.test.lm <- games.test3 %>%
  add_predictions(mod.lm.cumu,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0))
```

```{r}
table(games.test.lm$Home_Win, games.test.lm$class)
(err.lm.cumu <- with(games.test.lm, mean(Home_Win != class)))
```

```{r}
vip(mod.lm.cumu)
```

```{r}
res.vip5 <- vip(mod.lm.cumu,num_features=55)
vip.dat5 <- res.vip5[["data"]]
vip.dat5[29:32,]
```

```{r}
summary(mod.lm.cumu)
```

 *  The linear model displays an error rate of 39.4%, slightly better than our log model MSE. Important variables in our linear model include Home Strike Percentage, Away Walks and Hits per Inning Pitched (WHIP), and Away Batting Average Against. The R-Squared of these models is quite low, around 10%, most likely due to the difficulty of predicting these game outcomes from cumulative statistics.

### 5.b.iii) Lasso and Ridge Models

 *  First we must create new data matrices to use in these models.

```{r}
games.y3 <- data.matrix(games.train2$Home_Win)
games.test.y3 <- data.matrix(games.test2$Home_Win)

##New data set, gotta adjust the numbers selected
games.x3 <- games.train2 %>%
  select(4:21, 23:40)
games.x3[is.na(games.x3)] <- 0
games.x3 <- data.matrix(games.x3)

games.test.x3 <- games.test2 %>%
  select(4:21, 23:40)
games.test.x3[is.na(games.test.x3)] <- 0
games.test.x3 <- data.matrix(games.test.x3)
```


 *  From here, we can carry on as usual in building our ridge and lasso models.


```{r}
ridge.cv.cumu <- cv.glmnet(games.x3, games.y3,
                      family="binomial",
                      type.measure="class", 
                      alpha=0)
plot(ridge.cv.cumu)
```


```{r}
log(ridge.cv.cumu$lambda.1se)
log(ridge.cv.cumu$lambda.min)
```

```{r}
lambda.opt.cumu <- ridge.cv.cumu$lambda.1se
idrcumu <- with(ridge.cv.cumu,which(ridge.cv.cumu$lambda==lambda.opt.cumu))
(err.ridge.cumu <- ridge.cv.cumu$cvm[idrcumu])
```



```{r}
preds.ridge.cumu <- predict(ridge.cv.cumu, newx=games.test.x3,
                 type="class")
##The error rate on the testing dataset
table(games.test.y3 ,preds.ridge.cumu)
```

```{r}
vip(ridge.cv.cumu)
```

```{r}
res.vip6 <- vip(ridge.cv.cumu,num_features=55)
vip.dat6 <- res.vip6[["data"]]
vip.dat6[29:32,]
```

 *  We see an optimal lambda of 1.25 for our ridge model and an MSE of 39.42%. This is in between the error rates of the log and linear models. As previously seen in our ridge models, Batting Average is one of the most important variables, with On Base Percentage also very important. Again, we must note that the lasso and ridge models are completed through cross validation, so we can assume that the error rate may vary around 39% to 40% from trial to trial. Thus, it's also safe to assume that it's safer to use the linear or log model instead of ridge. 

 *  We'll look at the lasso model error to see if the MSE is significantly better there. 

```{r}
games.y3 <- data.matrix(games.train2$Home_Win)

##New data set, gotta adjust the numbers selected
games.x.lasso <- games.train2 %>%
  select(4:21, 23:40)
games.x.lasso[is.na(games.x.lasso)] <- 0
games.x3 <- data.matrix(games.x.lasso)
```

```{r}
lasso.cv.cumu <- cv.glmnet(games.x3,
                      games.y3,
                      family="binomial",
                      type.measure="class", 
                      alpha=1)

plot(lasso.cv.cumu)
```

```{r}
vip(lasso.cv.cumu)
```

```{r}
preds.lcumu <- predict(lasso.cv.cumu, newx=games.test.x3,
                 type="class")
##The error rate on the testing dataset
table(games.test.y3 ,preds.lcumu)
```

```{r}
log(lasso.cv.cumu$lambda.1se)
log(lasso.cv.cumu$lambda.1se)
```

```{r}
lambda.opt.l <- lasso.cv.cumu$lambda.1se
idlcumu <- with(lasso.cv.cumu,which(lasso.cv.cumu$lambda==lambda.opt.l))
(err.lasso.cumu <- lasso.cv.cumu$cvm[idlcumu])
vip(lasso.cv.cumu)
```

 *  For the lasso model, we see an optimal lambda of -2.96 with 6 explanatory variables maintained. This model leads to a 38.9% error rate, significantly better than our previous models.

 *  Note that optimal lasso and ridge models were considered but did not prove to provide better MSEs than the cross-validated versions for in-game or cumulative statistics. Thus we chose to remove these from our outline.

### 5.b.iv) Boosting

We also considered a number of more advanced machine learning models. First, we considered a boosting model. We left the distribution as "gaussian" as opposed to "bernoulli" so that our prediction not only gives us who we believe will win, we will get a probability of victory for the home team.

```{r}
numTrees <- 2000
theShrinkage <- 0.01
theDepth <- 2
mod.gbm <- gbm(Home_Win ~ .,
               data=games.train3,
               distribution="gaussian",
               n.trees=numTrees,
               shrinkage=theShrinkage,
               interaction.depth = theDepth)


games.test3$predGBM <- predict(mod.gbm,newdata=games.test3)
```

```{r}
games.test3.boost <- games.test3%>%
  mutate(pred = if_else(predGBM > 0.5, 1, 0))
table(games.test3.boost$Home_Win, games.test3.boost$pred)
(mean((games.test3.boost$Home_Win != games.test3.boost$pred)))
```

 *  Even with just an arbitrary choice for the number of trees, the depth, and the shrinkage parameter, we still see an error rate of around .383, which is right in our expected range, and is also arguably our best model thus far.


 *  However, we still want to see if we can improve our model. So, we can build a cross-validation function to optimize our parameters.

```{r}
games.test3 <- games.test3%>%
  select(1:37)

cvGBM <- function(data.df, theShrinkage, theDepth, numTrees, numFolds=5){
  N <- nrow(data.df)
  folds <- sample(1:numFolds,N,rep=T)
  errs <- numeric(numFolds)
  for(fold in 1:numFolds){
    train.df.cv <- data.df %>%
      filter(folds != fold)
    test.df.cv <- data.df %>%
      filter(folds == fold)
    mod.gbm <- gbm(Home_Win ~ .,
                   data=train.df.cv,
                   distribution="gaussian",
                   n.minobsinnode=10,
                   interaction.depth = theDepth,
                   shrinkage=theShrinkage,
                   n.trees=numTrees)
    test.df.cv$pred.gbm <- predict(mod.gbm,
                        newdata=test.df.cv,
                        n.trees=numTrees)
    test.df.cv <- test.df.cv%>%
      mutate(pred = if_else(pred.gbm > 0.5, 1, 0))
    errs[fold] <- with(test.df.cv,mean((Home_Win != pred)))
    }
  mean(errs)
}
```

```{r}
lambda <- 0.01
depth <- 1
numTrees <- 100
cvGBM(games.train3,lambda,depth,numTrees)
```

```{r}
cvGBM(games.train3,lambda,depth,100 * numTrees)
```


 *  We again see decent and expected error rates in spite of our arbitrarily chosen parameters for our boosting. While this second error rate is within expectation, we must be wary of an overfit with that many trees used.



 *  We can also use the built in cross validation function to try to get our optimal number of trees.


```{r}
numTrees <- 5000
mod.gbm.cv <- gbm(Home_Win ~ .,
                  data=games.train3,
                  distribution="gaussian",
                  n.trees=numTrees,
                  shrinkage=lambda,
                  interaction.depth = depth,
                  ##indicate folds here
                  cv.folds = 5,
                  ## restrict the minimum number of observations in a node
                  n.minobsinnode=10,
                  n.cores = 4) ## <-- use the cores on your computer
```

```{r}
gbm.best <- gbm.perf(mod.gbm.cv,method="cv")
(numTreesOpt <- gbm.best)
```


 *  With our optimal number of trees, we can fit an optimal model.


```{r}
mod.gbm.opt <- gbm(Home_Win ~ .,
                   data=games.train3,
                   distribution="gaussian", ## for regression
                   n.trees=numTreesOpt,
                   shrinkage=lambda,
                   interaction.depth = depth)
games.test3a <- games.test3 %>%
  add_predictions(mod.gbm.opt)

games.test3a <- games.test3a%>%
  mutate(class = if_else(pred > 0.5, 1, 0))
(mean(games.test3a$Home_Win != games.test3a$class))
```


 *  Not bad. However, we still haven't attempted to optimize our shrinkage or depth. Our next function does just that.


```{r}
shrink.vals <- c(0.1,0.01,0.001)
depth.vals <- c(1,2,3)
numTreesMax <- 5000
vals <- expand.grid(s=shrink.vals,d=depth.vals)
errs <- matrix(nrow=3,ncol=3)
i <- 1
errs <- numeric(9)
numTreesOpt <- numeric(9)
for(i in 1:9){
  lambda <- vals[i,1]
  depth <- vals[i,2]
  mod.gbm.cv <- gbm(Home_Win ~.,
                    data=games.train3,
                    distribution="gaussian",
                    n.trees=numTreesMax,
                    shrinkage=lambda,
                    interaction.depth = depth,
                    ##indicate folds here
                    cv.folds = 5,
                    ## restrict the minimum number of observations in a node
                    n.minobsinnode=10,
                    n.cores = 4)
gbm.best <- gbm.perf(mod.gbm.cv,method="cv")
numTreesOpt[i] <- gbm.best
errs[i] <- cvGBM(games.train3,
                 lambda,
                 depth,
                 numTreesOpt[i])
print(sprintf("shrink=%s, depth=%s, numTrees=%s, MSE=%s",lambda,depth,numTreesOpt[i],errs[i]))
}
```


```{r}
(err.opt = min(errs))
id <- which.min(errs)
(lambdaOpt <- vals[id,1])
(depthOpt <- vals[id,2])
(treesOpt <- numTreesOpt[i])
```


 *  From this model we take the shrinkage/depth/trees combination that yields the best cross-validation error rate to compute our optimal train/test error rate.

```{r}
mod.gbm.opt <- gbm(Home_Win ~ .,
                   data=games.train3,
                   distribution="gaussian", ## for regression
                   n.trees=treesOpt,
                   shrinkage=lambdaOpt,
                   interaction.depth = depthOpt)
```

```{r}
games.test3 %>%
  add_predictions(mod.gbm.opt) %>%
  mutate(class = if_else(pred > 0.5, 1, 0))%>%
  with(mean((Home_Win != class)))
```

```{r}
vip(mod.gbm.opt)
```

 *  This model yields an error rate of roughly .387 and has important variables that we've seen throughout the analysis, with On Base Plus Slugging, On Base Percentage, and WHIP showing high importance.

### 5.b.v) Bagging

Our next machine learning model we created was a bagging model. Since, as is the case with any boostrapping model, the higher the number of boots/trees the better, we set our number of trees to be 500 to give us a large number of trees without increasing our computational time too much.

```{r, warning = FALSE}
numTree <- 500
numPred <- ncol(games.train3)-1
set.seed(10)
mod.bag <- randomForest(as.factor(Home_Win) ~ .,
                        data=games.train3,
                        ntree=numTree,
                        mtry=numPred)

plot(mod.bag)
```


 *  Our plot shows the error rate oscillating between roughly .46 and .43, which is higher than we would expect. However, we will still fit our train/test model.


```{r}
vip(mod.bag)

res.vipx <- vip(mod.bag,num_features=55)
vip.datx <- res.vip2[["data"]]
vip.datx[52:55,]
```


 *  We see strikeouts having a much higher importance in this model, with Offensive Strikeout Percentage for both the home and away teams represented in the top 3 most important variables.


```{r}
bagtest.df <- games.test3 %>%
  add_predictions(mod.bag,
                  var="win.pred",
                  type = "class")

mse.bag <- bagtest.df %>%
  with(mean((Home_Win != win.pred)))

mse.bag

table(bagtest.df$Home_Win, bagtest.df$win.pred)
```

Unsurprisingly, based on our original plot, our bagging error rate, while still within our range, doesn't compete with the error rates of our other models.

### 5.b.vi) KNN


We chose to fit two different KNN models, one with one train/test dataset and one with multiple boostrapped train/test datasets. Unfortunately, with the size of our data, the dimensionality of our data made the KNN model almost unusable. For this algorithm to be able to compete in accuracy with our other models, we needed to use values of k of 100+, which became computationally unrealistic. 


```{r, warning=FALSE, eval=FALSE}
calc_one_MSE_knn <- function(kNear){
    train.df <- games.train3
    test.df <- games.test3

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}


M <- 1
kvalue <- 1
calc_one_MSE_knn_aux <- function(iter) {
  calc_one_MSE_knn(kvalue)
}
mse.vec <- map_dbl(1:M, calc_one_MSE_knn_aux)


M <- 1
kvalue <- 1
mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))


M <- 1
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mean <- mean(mse.vec)
  tibble(error = mean)
}


kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn)
mse.tbl <- mse.tbl %>%
  mutate(k=row_number())


ggplot(mse.tbl) +
  geom_line(aes(x=k, y=error), color="blue")+
  geom_smooth(aes(x=k, y=error), color="blue",se=F)

(mse.opt.k <- mse.tbl%>%
    filter(error == min(error))%>%
    filter(k == max(k)))
```




```{r, eval=FALSE}
numTrain <- nrow(games.train3) #Number of observations in training
numTest <- nrow(games.train3)  #Number of observations in testing

#Function to calculate the MSE for one simulation with a fixed kNear for KNN
calc_one_MSE_knn <- function(kNear){
    train.df <- sample_n(games.train3,numTrain, rep = T)
    test.df <- sample_n(games.test3,numTest, rep = T)

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}

M <- 100 #Number of simulation
#Function to calculate the mean and sd of MSE with a fixed kNear for KNN
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mse.mean <- mean(mse.vec)
  mse.sd <- sd(mse.vec)
  tibble(mse.mean=mse.mean, mse.sd=mse.sd)
}

kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn) #We calculate the mean,sd of MSE for all parameters

mse.tbl <- mse.tbl %>%
  mutate(k=row_number()) # We add the parameter (k)

# We plot the MSE bands as a function of k
ggplot(mse.tbl) +
  geom_line(aes(x=k, y=mse.mean), color="blue")+
  geom_smooth(aes(x=k, y=mse.mean), color="blue",se=F)+
  geom_line(aes(x=k, y=mse.mean-mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean-mse.sd), color="red", se=F)+
  geom_line(aes(x=k, y=mse.mean+mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean+mse.sd), color="red",se =F)

(mse.opt.boot <- mse.tbl%>%
    mutate(val = mse.mean - mse.sd)%>%
    filter(val <= mse.mean)%>%
    filter(k == max(k)))


opt.K <- mse.opt.boot$k

mod.knn.opt <- knn3(as.factor(Home_Win) ~.,
                              data = games.train3,
                              k = opt.K)
```






# 6) Alternate dataset


With 36 predictor variables, we wondered if there was a logical way to reduce that while still keeping all the information present. We decided to do so by examining the difference between the Home and Away teams for a given statistic. For example, instead of having the variable "Home_BA" for the home team's batting average and "Away_BA" for the away team's batting average, we have the variable "BA_Diff", which is the home team's batting average minus the away team's batting average. Along with these raw differences, we also considered finding the percentage difference between the two teams for each given statistic by taking the difference described previously and dividing by the home team's predictor.


```{r, include=FALSE}
games.train4 <- games.train2%>%
  mutate(BA_diff = Home_BA - Away_BA,
         OBP_diff = Home_OBP - Away_OBP,
         SLG_diff = Home_SLG - Away_SLG,
         OPS_diff = Home_OPS - Away_OPS,
         ISO_diff = Home_ISO - Away_ISO,
         SOPercH_diff = Home_SOPercH - Away_SOPercH,
         BAA_diff = Home_BAA - Away_BAA,
         OBPA_diff = Home_OBPA - Away_OBPA,
         SLGA_diff = Home_SLGA - Away_SLGA,
         OPSA_diff = Home_OPSA - Away_OPSA,
         StrPerc_diff = Home_StrPerc - Away_StrPerc,
         ERA_diff = Home_ERA - Away_ERA,
         WHIP_diff = Home_WHIP - Away_WHIP,
         FIP_diff = Home_FIP - Away_FIP,
         SOPerc_diff = Home_SOPerc - Away_SOPerc,
         KBB_diff = Home_KBB - Away_KBB,
         K9_diff = Home_K9 - Away_K9,
         HR9_diff = Home_HR9 - Away_HR9)

games.train5 <- games.train4%>%
  select(Date, Matchup, Home_Team, Away_Team, BA_diff, OBP_diff, SLG_diff, OPS_diff, ISO_diff, SOPercH_diff, BAA_diff, OBPA_diff, OPSA_diff, StrPerc_diff, ERA_diff, WHIP_diff, FIP_diff, SOPerc_diff, KBB_diff, K9_diff, HR9_diff, Home_Win, Home_RunDiff)

games.train6 <- games.train4%>%
  mutate(BA_percdiff = BA_diff/Home_BA,
         BA_percdiff = BA_diff/(Home_BA + 0.0001),
         OBP_percdiff = OBP_diff/(Home_OBP + 0.0001),
         SLG_percdiff = SLG_diff/(Home_SLG + 0.0001),
         OPS_percdiff = OPS_diff/(Home_OPS + 0.0001),
         ISO_percdiff = ISO_diff/(Home_ISO + 0.0001),
         SOPercH_percdiff = SOPercH_diff/(Home_SOPercH + 0.0001),
         BAA_percdiff = BAA_diff/(Home_BAA + 0.0001),
         OBPA_percdiff = OBPA_diff/(Home_OBPA + 0.0001),
         SLGA_percdiff = SLGA_diff/(Home_SLGA + 0.0001),
         OPSA_percdiff = OPSA_diff/(Home_OPSA + 0.0001),
         StrPerc_percdiff = StrPerc_diff/(Home_StrPerc + 0.0001),
         ERA_percdiff = ERA_diff/(Home_ERA + 0.0001),
         WHIP_percdiff = WHIP_diff/(Home_WHIP + 0.0001),
         FIP_percdiff = FIP_diff/(Home_FIP + 0.0001),
         SOPerc_percdiff = SOPerc_diff/(Home_SOPerc + 0.0001),
         KBB_percdiff = KBB_diff/(Home_KBB + 0.0001),
         K9_percdiff = K9_diff/(Home_K9 + 0.0001),
         HR9_percdiff = HR9_diff/(Home_HR9 + 0.0001))%>%
  select(Date, Matchup, Home_Team, Away_Team, BA_percdiff, OBP_percdiff, SLG_percdiff, OPS_percdiff, ISO_percdiff, SOPercH_percdiff, BAA_percdiff, OBPA_percdiff, OPSA_percdiff, StrPerc_percdiff, ERA_percdiff, WHIP_percdiff, FIP_percdiff, SOPerc_percdiff, KBB_percdiff, K9_percdiff, HR9_percdiff, Home_Win, Home_RunDiff)
  
```


```{r, include = FALSE}
games.test4 <- games.test2%>%
  mutate(BA_diff = Home_BA - Away_BA,
         OBP_diff = Home_OBP - Away_OBP,
         SLG_diff = Home_SLG - Away_SLG,
         OPS_diff = Home_OPS - Away_OPS,
         ISO_diff = Home_ISO - Away_ISO,
         SOPercH_diff = Home_SOPercH - Away_SOPercH,
         BAA_diff = Home_BAA - Away_BAA,
         OBPA_diff = Home_OBPA - Away_OBPA,
         SLGA_diff = Home_SLGA - Away_SLGA,
         OPSA_diff = Home_OPSA - Away_OPSA,
         StrPerc_diff = Home_StrPerc - Away_StrPerc,
         ERA_diff = Home_ERA - Away_ERA,
         WHIP_diff = Home_WHIP - Away_WHIP,
         FIP_diff = Home_FIP - Away_FIP,
         SOPerc_diff = Home_SOPerc - Away_SOPerc,
         KBB_diff = Home_KBB - Away_KBB,
         K9_diff = Home_K9 - Away_K9,
         HR9_diff = Home_HR9 - Away_HR9)

games.test5 <- games.test4%>%
  select(Date, Matchup, Home_Team, Away_Team, BA_diff, OBP_diff, SLG_diff, OPS_diff, ISO_diff, SOPercH_diff, BAA_diff, OBPA_diff, OPSA_diff, StrPerc_diff, ERA_diff, WHIP_diff, FIP_diff, SOPerc_diff, KBB_diff, K9_diff, HR9_diff, Home_Win, Home_RunDiff)

games.test6 <- games.test4%>%
  mutate(BA_percdiff = BA_diff/(Home_BA + 0.0001),
         OBP_percdiff = OBP_diff/(Home_OBP + 0.0001),
         SLG_percdiff = SLG_diff/(Home_SLG + 0.0001),
         OPS_percdiff = OPS_diff/(Home_OPS + 0.0001),
         ISO_percdiff = ISO_diff/(Home_ISO + 0.0001),
         SOPercH_percdiff = SOPercH_diff/(Home_SOPercH + 0.0001),
         BAA_percdiff = BAA_diff/(Home_BAA + 0.0001),
         OBPA_percdiff = OBPA_diff/(Home_OBPA + 0.0001),
         SLGA_percdiff = SLGA_diff/(Home_SLGA + 0.0001),
         OPSA_percdiff = OPSA_diff/(Home_OPSA + 0.0001),
         StrPerc_percdiff = StrPerc_diff/(Home_StrPerc + 0.0001),
         ERA_percdiff = ERA_diff/(Home_ERA + 0.0001),
         WHIP_percdiff = WHIP_diff/(Home_WHIP + 0.0001),
         FIP_percdiff = FIP_diff/(Home_FIP + 0.0001),
         SOPerc_percdiff = SOPerc_diff/(Home_SOPerc + 0.0001),
         KBB_percdiff = KBB_diff/(Home_KBB + 0.0001),
         K9_percdiff = K9_diff/(Home_K9 + 0.0001),
         HR9_percdiff = HR9_diff/(Home_HR9 + 0.0001))%>%
  select(Date, Matchup, Home_Team, Away_Team, BA_percdiff, OBP_percdiff, SLG_percdiff, OPS_percdiff, ISO_percdiff, SOPercH_percdiff, BAA_percdiff, OBPA_percdiff, OPSA_percdiff, StrPerc_percdiff, ERA_percdiff, WHIP_percdiff, FIP_percdiff, SOPerc_percdiff, KBB_percdiff, K9_percdiff, HR9_percdiff, Home_Win, Home_RunDiff)
  
```



## 6.a) Difference Variables


### 6.a.i) Log Model

```{r, warning = FALSE}
games.train7 <- games.train5 %>%
  select(5:22)

games.test7 <- games.test5 %>%
  select(5:22)

mod.log.diff <- glm(Home_Win ~.,
               data = games.train7,
               family = "binomial")

games.test7 <- games.test7 %>%
  add_predictions(mod.log.diff,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0)) #class instead of response

table(games.test7$Home_Win, games.test7$class)
(err.log.diff <- with(games.test7, mean(Home_Win != class)))
```


```{r}
vip(mod.log.diff)
```

Our error rate and important variables are quite similar to those of our regular log model, which is not surprising. This gives us a model with similar error but fewer predictors, which is preffered. 


### 6.a.ii) Lasso and Ridge

```{r}
games.y.diff <- data.matrix(games.train7$Home_Win)


##New data set, gotta adjust the numbers selected
games.x.diff <- games.train7 %>%
  select(1:17)
games.x.diff[is.na(games.x.diff)] <- 0
games.x.diff <- data.matrix(games.x.diff)
```

```{r}
ridge.cv.diff <- cv.glmnet(games.x.diff, games.y.diff,
                      family="binomial",
                      type.measure="class", 
                      alpha=0)
plot(ridge.cv.diff)
```


```{r}
log(ridge.cv.diff$lambda.1se)
log(ridge.cv.diff$lambda.min)
```

```{r}
lambda.opt.diff <- ridge.cv.diff$lambda.1se
id <- with(ridge.cv.diff,which(ridge.cv.diff$lambda==lambda.opt.diff))
(err.ridge.diff <- ridge.cv.diff$cvm[id])
```




```{r}
mod.ridge.opt.diff <- glmnet(games.x.diff, games.y.diff,
                        family="binomial",
                        type.measure="class",
                        alpha=0,
                        lambda=lambda.opt.diff)

##New data set, gotta adjust the numbers selected
games.test.y.diff <- data.matrix(games.test7$Home_Win)
games.test.x.diff <- games.test7 %>%
  select(1:17)
games.test.x.diff[is.na(games.test.x.diff)] <- 0
games.test.x.diff <- data.matrix(games.test.x.diff)
```

```{r}
preds.ridge.diff <- predict(mod.ridge.opt.diff, newx=games.test.x.diff,
                 type="class")
##The error rate on the testing dataset
table(games.test.y.diff, preds.ridge.diff)
```

```{r}
(err.ridge.opt.diff <- mean((games.test.y.diff != preds.ridge.diff)))
```

```{r}
vip(mod.ridge.opt.diff)
```


As with the log model, the ridge model has an error rate similar to the original ridge model of roughly .389. Also similar to our other ridge models, we see that predictors using Batting Average are the most important.


```{r}
lasso.cv.diff <- cv.glmnet(games.x.diff,
                      games.y.diff,
                      family="binomial",
                      type.measure="class", 
                      alpha=1)

plot(lasso.cv.diff)
```

```{r}
log(lasso.cv.diff$lambda.1se)
log(lasso.cv.diff$lambda.min)
```

```{r}
lambda.opt.diff <- lasso.cv.diff$lambda.1se
id <- with(lasso.cv.diff,which(lasso.cv.diff$lambda==lambda.opt.diff))
(err.lasso.diff <- lasso.cv.diff$cvm[id])
```

 *  This cross-validation error rate for the lasso model is by far our best one yet at .381.


```{r}
mod.lasso.opt.diff <- glmnet(games.x.diff, games.y.diff,
                        family="binomial",
                        type.measure="class",
                        alpha=1,
                        lambda=lambda.opt.diff)


```

```{r}
preds.lasso.diff <- predict(mod.lasso.opt.diff, newx=games.test.x.diff,
                 type="class")
##The error rate on the testing dataset
table(games.test.y.diff ,preds.lasso.diff)
```

```{r}
(err.lasso.opt.diff <- mean((games.test.y.diff != preds.lasso.diff)))
```

```{r}
vip(mod.lasso.opt.diff)
```

 * Our train/test lasso model doesn't quite perform as well as the cross-validation model. However, we still see a reasonable error rate of .387, as well as seeing the same trend of repeated important variables, with On Base Plus Slugging still the most important.

### 6.a.iii) Boosting

We carry out the same process as above with the boosting model.

```{r, warning = FALSE}
numTrees <- 2000
theShrinkage <- 0.01
theDepth <- 2
mod.gbm.diff <- gbm(Home_Win ~ .,
               data=games.train7,
               distribution="gaussian",
               n.trees=numTrees,
               shrinkage=theShrinkage,
               interaction.depth = theDepth)

games.test7$predGBM <- predict(mod.gbm.diff,newdata=games.test7)
```

```{r}
games.test7.boost <- games.test7%>%
  mutate(pred = if_else(predGBM > 0.5, 1, 0))
table(games.test7.boost$Home_Win, games.test7.boost$pred)
(mean((games.test7.boost$Home_Win != games.test7.boost$pred)))
```

 *  The error rate for this arbitrary shrinkage, depth, and number of trees isn't as nice as our previous arbitrary values at 0.394. However, it still falls within our expected performance range.


```{r}
games.test7 <- games.test7%>%
  select(1:18)

cvGBM <- function(data.df, theShrinkage, theDepth, numTrees, numFolds=5){
  N <- nrow(data.df)
  folds <- sample(1:numFolds,N,rep=T)
  errs <- numeric(numFolds)
  for(fold in 1:numFolds){
    train.df.cv <- data.df %>%
      filter(folds != fold)
    test.df.cv <- data.df %>%
      filter(folds == fold)
    mod.gbm <- gbm(Home_Win ~ .,
                   data=train.df.cv,
                   distribution="gaussian",
                   n.minobsinnode=10,
                   interaction.depth = theDepth,
                   shrinkage=theShrinkage,
                   n.trees=numTrees)
    test.df.cv$pred.gbm <- predict(mod.gbm,
                        newdata=test.df.cv,
                        n.trees=numTrees)
    test.df.cv <- test.df.cv%>%
      mutate(pred = if_else(pred.gbm > 0.5, 1, 0))
    errs[fold] <- with(test.df.cv,mean((Home_Win != pred)))
    }
  mean(errs)
}
```

```{r}
lambda <- 0.01
depth <- 1
numTrees <- 100
cvGBM(games.train7,lambda,depth,numTrees)
```

```{r}
cvGBM(games.train7,lambda,depth,100 * numTrees)
```

 
 *  Our cross-validation function performs slightly better than our raw model. Again, we see the issue with overfitting the boosted model, with the error rate jumpin over .4.


```{r}
numTrees <- 5000
mod.gbm.cv.diff <- gbm(Home_Win ~ .,
                  data=games.train7,
                  distribution="gaussian",
                  n.trees=numTrees,
                  shrinkage=lambda,
                  interaction.depth = depth,
                  ##indicate folds here
                  cv.folds = 5,
                  ## restrict the minimum number of observations in a node
                  n.minobsinnode=10,
                  n.cores = 4) ## <-- use the cores on your computer
```

```{r}
gbm.best.diff <- gbm.perf(mod.gbm.cv.diff,method="cv")
(numTreesOpt.diff <- gbm.best.diff)
```

```{r, warning = FALSE}
mod.gbm.opt.diff <- gbm(Home_Win ~ .,
                   data=games.train7,
                   distribution="gaussian", ## for regression
                   n.trees=numTreesOpt.diff,
                   shrinkage=lambda,
                   interaction.depth = depth)
games.test7a <- games.test7 %>%
  add_predictions(mod.gbm.opt.diff)

games.test7a <- games.test7a%>%
  mutate(class = if_else(pred > 0.5, 1, 0))
(mean(games.test7a$Home_Win != games.test7a$class))
```


 *  This optimal train/test model is our best train/test model thus far, tied with the previous boosting model, with an error of 0.383.


 *  We can carry out the same process as above to optimize our other parameters.


```{r}
shrink.vals <- c(0.1,0.01,0.001)
depth.vals <- c(1,2,3)
numTreesMax <- 5000
vals <- expand.grid(s=shrink.vals,d=depth.vals)
errs <- matrix(nrow=3,ncol=3)
i <- 1
errs <- numeric(9)
numTreesOptDiff <- numeric(9)
for(i in 1:9){
  lambda <- vals[i,1]
  depth <- vals[i,2]
  mod.gbm.cv.diff2 <- gbm(Home_Win ~.,
                    data=games.train7,
                    distribution="gaussian",
                    n.trees=numTreesMax,
                    shrinkage=lambda,
                    interaction.depth = depth,
                    ##indicate folds here
                    cv.folds = 5,
                    ## restrict the minimum number of observations in a node
                    n.minobsinnode=10,
                    n.cores = 4)
gbm.best.diff2 <- gbm.perf(mod.gbm.cv.diff2,method="cv")
numTreesOptDiff[i] <- gbm.best.diff2
errs[i] <- cvGBM(games.train7,
                 lambda,
                 depth,
                 numTreesOptDiff[i])
print(sprintf("shrink=%s, depth=%s, numTrees=%s, MSE=%s",lambda,depth, numTreesOptDiff[i],errs[i]))
}
```

```{r}
(err.opt = min(errs))
id <- which.min(errs)
(lambdaOpt <- vals[id,1])
(depthOpt <- vals[id,2])
(treesOpt <- numTreesOptDiff[i])
```


```{r}
mod.gbm.optDiff <- gbm(Home_Win ~ .,
                   data=games.train7,
                   distribution="gaussian", ## for regression
                   n.trees=treesOpt,
                   shrinkage=lambdaOpt,
                   interaction.depth = depthOpt)
```

```{r}
games.test7 %>%
  add_predictions(mod.gbm.optDiff) %>%
  mutate(class = if_else(pred > 0.5, 1, 0))%>%
  with(mean((Home_Win != class)))
```

 *  For some reason, this model doesn't perform nearly as well as our previous model, and is our only model that doesn't fit within our expected range with an error rate of 0.434.

```{r}
vip(mod.gbm.optDiff)
```

 *  Potentially one of the reasons this model doesn't perform as well as the others is the variable selection. Home runs allowed per 9 innings played is one of the most important variables, but isn't included anywhere in the other models.

### 6.a.iv) Bagging

Although this was our worst performing model with the previous dataset, it is still worthwhile to see how this model will perform on the new dataset.

```{r, warning = FALSE}
numTree <- 500
numPred <- ncol(games.train7)-1
set.seed(10)
mod.bag.diff <- randomForest(as.factor(Home_Win) ~ .,
                        data=games.train7,
                        ntree=numTree,
                        mtry=numPred)

plot(mod.bag.diff)
```



```{r}
vip(mod.bag.diff)

res.vipx <- vip(mod.bag,num_features=55)
vip.datx <- res.vip2[["data"]]
vip.datx[52:55,]
```

```{r}
bagtest.diff.df <- games.test7 %>%
  add_predictions(mod.bag.diff,
                  var="win.pred",
                  type = "class")

mse.bag.diff <- bagtest.diff.df %>%
  with(mean((Home_Win != win.pred)))

mse.bag.diff

table(bagtest.diff.df$Home_Win, bagtest.diff.df$win.pred)
```


 *  We again see the trend of similar error rates and similar important variables, with an error rate of 0.39 and offensive strikeout rate being one of the most important variables. However, On Base Plus Slugging, the important variable from our arguably best model lasso, is now near the top.


## 6.a.iv)KNN

 *  As before, the KNN model needed too much computational power to be realistic in this instance.

```{r, eval = FALSE, include = FALSE}
calc_one_MSE_knn <- function(kNear){
    train.df <- games.train7
    test.df <- games.test7

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}


M <- 1
kvalue <- 1
calc_one_MSE_knn_aux <- function(iter) {
  calc_one_MSE_knn(kvalue)
}
mse.vec <- map_dbl(1:M, calc_one_MSE_knn_aux)


M <- 1
kvalue <- 1
mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))


M <- 1
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mean <- mean(mse.vec)
  tibble(error = mean)
}


kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn)
mse.tbl <- mse.tbl %>%
  mutate(k=row_number())


ggplot(mse.tbl) +
  geom_line(aes(x=k, y=error), color="blue")+
  geom_smooth(aes(x=k, y=error), color="blue",se=F)

(mse.opt.k <- mse.tbl%>%
    filter(error == min(error))%>%
    filter(k == max(k)))
```




```{r, eval = FALSE, include = FALSE}
numTrain <- nrow(games.train7) #Number of observations in training
numTest <- nrow(games.train7)  #Number of observations in testing

#Function to calculate the MSE for one simulation with a fixed kNear for KNN
calc_one_MSE_knn <- function(kNear){
    train.df <- sample_n(games.train7,numTrain, rep = T)
    test.df <- sample_n(games.test7,numTest, rep = T)

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}

M <- 100 #Number of simulation
#Function to calculate the mean and sd of MSE with a fixed kNear for KNN
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mse.mean <- mean(mse.vec)
  mse.sd <- sd(mse.vec)
  tibble(mse.mean=mse.mean, mse.sd=mse.sd)
}

kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn) #We calculate the mean,sd of MSE for all parameters

mse.tbl <- mse.tbl %>%
  mutate(k=row_number()) # We add the parameter (k)

# We plot the MSE bands as a function of k
ggplot(mse.tbl) +
  geom_line(aes(x=k, y=mse.mean), color="blue")+
  geom_smooth(aes(x=k, y=mse.mean), color="blue",se=F)+
  geom_line(aes(x=k, y=mse.mean-mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean-mse.sd), color="red", se=F)+
  geom_line(aes(x=k, y=mse.mean+mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean+mse.sd), color="red",se =F)

(mse.opt.boot <- mse.tbl%>%
    mutate(val = mse.mean - mse.sd)%>%
    filter(val <= mse.mean)%>%
    filter(k == max(k)))


opt.K <- mse.opt.boot$k

mod.knn.opt <- knn3(as.factor(Home_Win) ~.,
                              data = games.train7,
                              k = opt.K)
```




## 6b) Percentage Difference Variables


### 6.b.i) Log Model

This dataset involves the percentage differences in home and away statistics. We will again begin by fitting the log model.

```{r, warning = FALSE}
games.train8 <- games.train6 %>%
  select(5:22)

games.test8 <- games.test6 %>%
  select(5:22)

mod.log.perc <- glm(Home_Win ~.,
               data = games.train8,
               family = "binomial")

games.test8 <- games.test8 %>%
  add_predictions(mod.log.perc,
                  type = "response") %>%
  mutate(class = if_else(pred > 0.5, 1,0)) #class instead of response

table(games.test8$Home_Win, games.test8$class)
(err.log.perc <- with(games.test8, mean(Home_Win != class)))
```


```{r}
vip(mod.log.perc)
```


 *  Somewhat surprisingly, the error rate of this log model got slightly worse. We also see considerably different importatn variables, with 3 pitching statistics, Fielding Independant Pitching, Strikeout Percentage, and Strikeouts per 9 Innings being the top predictors.


### 6.b.ii) Ridge and Lasso

```{r}
games.y.perc <- data.matrix(games.train8$Home_Win)


##New data set, gotta adjust the numbers selected
games.x.perc <- games.train8 %>%
  select(1:17)
games.x.perc[is.na(games.x.perc)] <- 0
games.x.perc <- data.matrix(games.x.perc)
```

```{r}
ridge.cv.perc <- cv.glmnet(games.x.perc, games.y.perc,
                      family="binomial",
                      type.measure="class", 
                      alpha=0)
plot(ridge.cv.perc)
```


```{r}
log(ridge.cv.perc$lambda.1se)
log(ridge.cv.perc$lambda.min)
```

```{r}
lambda.opt <- ridge.cv.perc$lambda.1se
id <- with(ridge.cv.perc,which(ridge.cv.perc$lambda==lambda.opt))
(err.ridge.perc <- ridge.cv.perc$cvm[id])
```

```{r}
mod.ridge.opt.perc <- glmnet(games.x.perc, games.y.perc,
                        family="binomial",
                        type.measure="class",
                        alpha=0,
                        lambda=lambda.opt)

##New data set, gotta adjust the numbers selected
games.test.y.perc <- data.matrix(games.test8$Home_Win)
games.test.x.perc <- games.test8 %>%
  select(1:17)
games.test.x.perc[is.na(games.test.x.perc)] <- 0
games.test.x.perc <- data.matrix(games.test.x.perc)
```

```{r}
preds.perc <- predict(mod.ridge.opt.perc, 
                      newx=games.test.x.perc,
                 type="class")
##The error rate on the testing dataset
table(games.test.y.perc ,preds.perc)
```

```{r}
(err.ridge.opt.perc <- mean((games.test.y.perc != preds.perc)))
```

```{r}
vip(mod.ridge.opt.perc)
```


 *  Again, we see our models performing worse, with our ridge model having an error rate of .39. Also again, we see different important variables, with Batting Average moving down to the 6th most important variable, and On Base Percentage and On Base Plus Slugging becoming the most important.


```{r}
lasso.cv.perc <- cv.glmnet(games.x.perc,
                      games.y.perc,
                      family="binomial",
                      type.measure="class", 
                      alpha=1)

plot(lasso.cv.perc)
```

```{r}
vip(lasso.cv.perc)
```

```{r}
log(lasso.cv.perc$lambda.1se)
log(lasso.cv.perc$lambda.min)
```

```{r}
lambda.opt <- lasso.cv.perc$lambda.1se
id <- with(lasso.cv.perc,which(lasso.cv.perc$lambda==lambda.opt))
(err.lasso.perc <- lasso.cv.perc$cvm[id])
```

```{r}
mod.lasso.opt.perc <- glmnet(games.x.perc, games.y.perc,
                        family="binomial",
                        type.measure="class",
                        alpha=1,
                        lambda=lambda.opt)

```

```{r}
preds.perc <- predict(mod.lasso.opt.perc, newx=games.test.x.perc,
                 type="class")
##The error rate on the testing dataset
table(games.test.y.perc ,preds.perc)
```

```{r}
(err.lasso.opt.perc <- mean((games.test.y.perc != preds.perc)))
```

```{r}
vip(mod.lasso.opt.perc)
```

 *  This cross-validation error rate of 0.380 is definitely our best one yet. In our train/test model, we see similar performance, with an error rate of .387. We also see similar important variables here, with On Base Plus Slugging for and against being the most important variables in all our lasso models.


### 6.b.iii) Boosting


```{r}
numTrees <- 2000
theShrinkage <- 0.01
theDepth <- 2
mod.gbm.diff <- gbm(Home_Win ~ .,
               data=games.train8,
               distribution="gaussian",
               n.trees=numTrees,
               shrinkage=theShrinkage,
               interaction.depth = theDepth)

games.test8$predGBM <- predict(mod.gbm.diff,newdata=games.test8)
```

```{r}
games.test8.boost <- games.test8%>%
  mutate(pred = if_else(predGBM > 0.5, 1, 0))
table(games.test8.boost$Home_Win, games.test8.boost$pred)
(mean((games.test8.boost$Home_Win != games.test8.boost$pred)))
```


```{r}
games.test8 <- games.test8%>%
  select(1:18)

cvGBM <- function(data.df, theShrinkage, theDepth, numTrees, numFolds=5){
  N <- nrow(data.df)
  folds <- sample(1:numFolds,N,rep=T)
  errs <- numeric(numFolds)
  for(fold in 1:numFolds){
    train.df.cv <- data.df %>%
      filter(folds != fold)
    test.df.cv <- data.df %>%
      filter(folds == fold)
    mod.gbm <- gbm(Home_Win ~ .,
                   data=train.df.cv,
                   distribution="gaussian",
                   n.minobsinnode=10,
                   interaction.depth = theDepth,
                   shrinkage=theShrinkage,
                   n.trees=numTrees)
    test.df.cv$pred.gbm <- predict(mod.gbm,
                        newdata=test.df.cv,
                        n.trees=numTrees)
    test.df.cv <- test.df.cv%>%
      mutate(pred = if_else(pred.gbm > 0.5, 1, 0))
    errs[fold] <- with(test.df.cv,mean((Home_Win != pred)))
    }
  mean(errs)
}
```

```{r}
lambda <- 0.01
depth <- 1
numTrees <- 100
cvGBM(games.train8,lambda,depth,numTrees)
```

```{r}
cvGBM(games.train8,lambda,depth,100 * numTrees)
```

```{r}
numTrees <- 5000
mod.gbm.cv.perc <- gbm(Home_Win ~ .,
                  data=games.train8,
                  distribution="gaussian",
                  n.trees=numTrees,
                  shrinkage=lambda,
                  interaction.depth = depth,
                  ##indicate folds here
                  cv.folds = 5,
                  ## restrict the minimum number of observations in a node
                  n.minobsinnode=10,
                  n.cores = 4) ## <-- use the cores on your computer
```

```{r}
gbm.best.perc <- gbm.perf(mod.gbm.cv.perc,method="cv")
(numTreesOpt.perc <- gbm.best.perc)
```

```{r}
mod.gbm.opt.perc <- gbm(Home_Win ~ .,
                   data=games.train8,
                   distribution="gaussian", ## for regression
                   n.trees=numTreesOpt.perc,
                   shrinkage=lambda,
                   interaction.depth = depth)
games.test7a <- games.test8 %>%
  add_predictions(mod.gbm.opt.perc)

games.test7a <- games.test7a%>%
  mutate(class = if_else(pred > 0.5, 1, 0))
(mean(games.test7a$Home_Win != games.test7a$class))
```


```{r}
shrink.vals <- c(0.1,0.01,0.001)
depth.vals <- c(1,2,3)
numTreesMax <- 5000
vals <- expand.grid(s=shrink.vals,d=depth.vals)
errs <- matrix(nrow=3,ncol=3)
i <- 1
errs <- numeric(9)
numTreesOptPerc <- numeric(9)
for(i in 1:9){
  lambda <- vals[i,1]
  depth <- vals[i,2]
  mod.gbm.cv.perc2 <- gbm(Home_Win ~.,
                    data=games.train8,
                    distribution="gaussian",
                    n.trees=numTreesMax,
                    shrinkage=lambda,
                    interaction.depth = depth,
                    ##indicate folds here
                    cv.folds = 5,
                    ## restrict the minimum number of observations in a node
                    n.minobsinnode=10,
                    n.cores = 4)
gbm.best.perc2 <- gbm.perf(mod.gbm.cv.perc2,method="cv")
numTreesOptPerc[i] <- gbm.best.perc2
errs[i] <- cvGBM(games.train8,
                 lambda,
                 depth,
                 numTreesOptPerc[i])
print(sprintf("shrink=%s, depth=%s, numTrees=%s, MSE=%s",lambda,depth, numTreesOptPerc[i],errs[i]))
}
```

```{r}
(err.opt = min(errs))
id <- which.min(errs)
(lambdaOpt <- vals[id,1])
(depthOpt <- vals[id,2])
(treesOpt <- numTreesOptPerc[i])
```


```{r}
mod.gbm.optPerc <- gbm(Home_Win ~ .,
                   data=games.train8,
                   distribution="gaussian", ## for regression
                   n.trees=treesOpt,
                   shrinkage=lambdaOpt,
                   interaction.depth = depthOpt)
```

```{r}
games.test8 %>%
  add_predictions(mod.gbm.optPerc) %>%
  mutate(class = if_else(pred > 0.5, 1, 0))%>%
  with(mean((Home_Win != class)))
```

```{r}
vip(mod.gbm.optPerc)
```


 *  After carrying out the boosting process one last time, we saw again that the boosting model performs very well in trying to predict these outcomes. Our error rate of .383 using our arbitrary number of trees and cross-validation compared nicely to our optimal error rate in a train/test model of 0.384. We again see On Base Plus Slugging as an important variable, with ERA also showing major importance.



### 6.b.iv) Bagging


```{r, warning = FALSE}
numTree <- 500
numPred <- ncol(games.train3)-1
set.seed(10)
mod.bag.perc <- randomForest(as.factor(Home_Win) ~ .,
                        data=games.train8,
                        ntree=numTree,
                        mtry=numPred)

plot(mod.bag.perc)
```



```{r}
vip(mod.bag.perc)

res.vipx <- vip(mod.bag.perc,num_features=55)
vip.datx <- res.vip2[["data"]]
vip.datx[52:55,]
```

```{r}
bagtest.perc.df <- games.test8 %>%
  add_predictions(mod.bag.perc,
                  var="win.pred",
                  type = "class")

mse.bag.perc <- bagtest.perc.df %>%
  with(mean((Home_Win != win.pred)))

mse.bag.perc

table(bagtest.perc.df$Home_Win, bagtest.perc.df$win.pred)
```


 * After one more iteration of our bagging model, we still see that the bagging model performs worse than all our other models, with error rates above .39. We also see the continued trend of offensive strikeout percentage as one of the most important variables in the bagging model.

### 6.b.v) KNN

Even though we reduced the number of predictors, the dimensionality of the dataset was still too much for the KNN model to be computationally reasonable for its performance level.

```{r, eval = FALSE}
calc_one_MSE_knn <- function(kNear){
    train.df <- games.train8
    test.df <- games.test8

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}


M <- 1
kvalue <- 1
calc_one_MSE_knn_aux <- function(iter) {
  calc_one_MSE_knn(kvalue)
}
mse.vec <- map_dbl(1:M, calc_one_MSE_knn_aux)


M <- 1
kvalue <- 1
mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))


M <- 1
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mean <- mean(mse.vec)
  tibble(error = mean)
}



kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn)
mse.tbl <- mse.tbl %>%
  mutate(k=row_number())


ggplot(mse.tbl) +
  geom_line(aes(x=k, y=error), color="blue")+
  geom_smooth(aes(x=k, y=error), color="blue",se=F)

(mse.opt.k <- mse.tbl%>%
    filter(error == min(error))%>%
    filter(k == max(k)))
```




```{r, eval = FALSE}
numTrain <- nrow(games.train8) #Number of observations in training
numTest <- nrow(games.train8)  #Number of observations in testing

#Function to calculate the MSE for one simulation with a fixed kNear for KNN
calc_one_MSE_knn <- function(kNear){
    train.df <- sample_n(games.train8,numTrain, rep = T)
    test.df <- sample_n(games.test8,numTest, rep = T)

    mod.knn <- knn3(as.factor(Home_Win) ~ .,
                    data=train.df,
                    k=kNear)
    
    test.df <- test.df %>% 
      add_predictions(mod.knn,
                      type="class",
                      var="knnClass")
    
    with(test.df,mean(Home_Win != knnClass))
}

M <- 100 #Number of simulation
#Function to calculate the mean and sd of MSE with a fixed kNear for KNN
calc_MSE_knn<- function(kvalue) {
  mse.vec <- map_dbl(1:M, ~calc_one_MSE_knn(kvalue))
  mse.mean <- mean(mse.vec)
  mse.sd <- sd(mse.vec)
  tibble(mse.mean=mse.mean, mse.sd=mse.sd)
}

kMax <- 100
mse.tbl <-map_df(1:kMax, calc_MSE_knn) #We calculate the mean,sd of MSE for all parameters

mse.tbl <- mse.tbl %>%
  mutate(k=row_number()) # We add the parameter (k)

# We plot the MSE bands as a function of k
ggplot(mse.tbl) +
  geom_line(aes(x=k, y=mse.mean), color="blue")+
  geom_smooth(aes(x=k, y=mse.mean), color="blue",se=F)+
  geom_line(aes(x=k, y=mse.mean-mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean-mse.sd), color="red", se=F)+
  geom_line(aes(x=k, y=mse.mean+mse.sd), color="red")+
  geom_smooth(aes(x=k, y=mse.mean+mse.sd), color="red",se =F)

(mse.opt.boot <- mse.tbl%>%
    mutate(val = mse.mean - mse.sd)%>%
    filter(val <= mse.mean)%>%
    filter(k == max(k)))


opt.K <- mse.opt.boot$k

mod.knn.opt <- knn3(as.factor(Home_Win) ~.,
                              data = games.train8,
                              k = opt.K)
```



# 7 Conclusions

Our boosting models continually performed as good or better than all our other models, with our lasso models coming in a close second. The bagging models routinely performed worse than all our other models.

Throughout the analysis, we see that our models performed better in accurately predicting wins than they did in accurately predicting losses. This could be do in large part to the random aspects of the game of baseball, where even if a team is completely overmatched in every major statistical category going into a game, an underdog team will win roughly 30% of the time. 

While our models performed comparably to other models present in mainstream sports betting or sports mathematical literature, most of which use more statistical and theoretical methods as opposed to our machine learning methods, we can still improve. One way to improve would be to include starting pitcher. Our model does not account for starting pitcher, which can greatly influence the outcome of a particular game, moreso than they can impact a teams overall statistical output or overall team record.














































